# 中文文本挖掘预处理

---

在对文本做数据分析时，我们一大半的时间都会花在文本预处理上，而中文和英文的预处理流程稍有不同，本文就对中文文本挖掘的预处理流程做一个总结。

## 1. 中文文本挖掘预处理特点

首先我们看看中文文本挖掘预处理和英文文本挖掘预处理相比的一些特殊点。

首先，中文文本是没有像英文的单词空格那样隔开的，因此不能直接像英文一样可以直接用最简单的空格和标点符号完成分词。所以一般我们需要用分词算法来完成分词，在[文本挖掘的分词原理](/nlp/text-mine.md)中，我们已经讲到了中文的分词原理，这里就不多说。

第二，中文的编码不是utf8，而是unicode。这样会导致在分词的时候，和英文相比，我们要处理编码的问题。

这两点构成了中文分词相比英文分词的一些不同点，后面我们也会重点讲述这部分的处理。当然，英文分词也有自己的烦恼，这个我们在以后再讲。了解了中文预处理的一些特点后，我们就言归正传，通过实践总结下中文文本挖掘预处理流程。

## 2.  中文文本挖掘预处理一：数据收集

在文本挖掘之前，我们需要得到文本数据，文本数据的获取方法一般有两种：使用别人做好的语料库和自己用爬虫去在网上去爬自己的语料数据。

对于第一种方法，常用的文本语料库在网上有很多，如果大家只是学习，则可以直接下载下来使用，但如果是某些特殊主题的语料库，比如“机器学习”相关的语料库，则这种方法行不通，需要我们自己用第二种方法去获取。


## 3.  中文文本挖掘预处理二：除去数据中非文本部分

这一步主要是针对我们用爬虫收集的语料数据，由于爬下来的内容中有很多html的一些标签，需要去掉。少量的非文本内容的可以直接用Python的正则表达式\(re\)删除, 复杂的则可以用[beautifulsoup](http://link.zhihu.com/?target=http%3A//www.crummy.com/software/BeautifulSoup/)来去除。去除掉这些非文本的内容后，我们就可以进行真正的文本预处理了。

## 4. 中文文本挖掘预处理三：处理中文编码问题

由于Python2不支持unicode的处理，因此我们使用Python2做中文文本预处理时需要遵循的原则是，存储数据都用utf8，读出来进行中文相关处理时，使用GBK之类的中文编码，在下面一节的分词时，我们再用例子说明这个问题。

## 5. 中文文本挖掘预处理四：中文分词

常用的中文分词软件有很多，个人比较推荐结巴分词。安装也很简单，比如基于Python的，用"pip install jieba"就可以完成。下面我们就用例子来看看如何中文分词。



可见分词效果还不错。

## 6. 中文文本挖掘预处理五：引入停用词

在上面我们解析的文本中有很多无效的词，比如“着”，“和”，还有一些标点符号，这些我们不想在文本分析的时候引入，因此需要去掉，这些词就是停用词。常用的中文停用词表是1208个。当然也有其他版本的停用词表，不过这个1208词版是我常用的。

在我们用scikit-learn做特征处理的时候，可以通过参数stop\_words来引入一个数组作为停用词表。

现在我们将停用词表从文件读出，并切分成一个数组备用：

```
#从文件导入停用词表
stpwrdpath = "stop_words.txt"
stpwrd_dic = open(stpwrdpath, 'rb')
stpwrd_content = stpwrd_dic.read()
#将停用词表转换为list  
stpwrdlst = stpwrd_content.splitlines()
stpwrd_dic.close()
```

## 7. 中文文本挖掘预处理六：特征处理

现在我们就可以用scikit-learn来对我们的文本特征进行处理了，在[文本挖掘预处理之向量化与Hash Trick](http://www.cnblogs.com/pinard/p/6688348.html)中，我们讲到了两种特征处理的方法，向量化与Hash Trick。而向量化是最常用的方法，因为它可以接着进行TF-IDF的特征处理。在[文本挖掘预处理之TF-IDF](http://www.cnblogs.com/pinard/p/6693230.html)中，我们也讲到了TF-IDF特征处理的方法。这里我们就用scikit-learn的TfidfVectorizer类来进行TF-IDF特征处理。

TfidfVectorizer类可以帮助我们完成向量化，TF-IDF和标准化三步。当然，还可以帮我们处理停用词。



## 8. 中文文本挖掘预处理七：建立分析模型

有了每段文本的TF-IDF的特征向量，我们就可以利用这些数据建立分类模型，或者聚类模型了，或者进行主题模型的分析。比如我们上面的两段文本，就可以是两个训练样本了。此时的分类聚类模型和之前讲的非自然语言处理的数据分析没有什么两样。因此对应的算法都可以直接使用。而主题模型是自然语言处理比较特殊的一块，这个我们后面再单独讲。

## 9.中文文本挖掘预处理总结

上面我们对中文文本挖掘预处理的过程做了一个总结，希望可以帮助到大家。需要注意的是这个流程主要针对一些常用的文本挖掘，并使用了词袋模型，对于某一些自然语言处理的需求则流程需要修改。比如我们涉及到词上下文关系的一些需求，此时不能使用词袋模型。而有时候我们对于特征的处理有自己的特殊需求，因此这个流程仅供自然语言处理入门者参考。